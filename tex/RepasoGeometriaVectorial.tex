\chapter{Repaso de geometría vectorial}
\label{geovec}
\section{Algunos resultados de álgebra matricial}
POR HACER
\section{Coordenadas y cambios de base}
\label{geovec_coordenadas}
Sea $E$ un $\K$--espacio vectorial de dimensión $n$. Asimismo sea $\mc{B}:=\{e_1,\dots,e_n\}$ una base de $E$. Es claro (compruébese) que todo vector $x\in E$ puede escribirse de manera única como combinación lineal de los vectores de la base $\mc{B}$. Escrito con menos literatura, hay unos únicos $\alpha_1,\dots,\alpha_n\in\K$ de manera que
\begin{equation}
	\label{geovec_eq_combinacionLineal}
	x=\alpha_1e_1+\dots+\alpha_ne_n
\end{equation}
De esta forma, usualmente se suele adoptar la notación $x=(\alpha_1,\dots,\alpha_n)_{\mc{B}}$. A esto lo llamamos \ti{escritura de $x$ en coordenadas de $\mc{B}$}. Escribiendo la ecuación \eqref{geovec_eq_combinacionLineal} de forma matricial, obtenemos
\begin{equation}
	\label{geovec_eq_combinacionLinealMatrices}
	x=\begin{pmatrix}
	e_1 & \cdots & e_n
	\end{pmatrix}\begin{pmatrix}
	\alpha_1\\
	\cdots\\
	\alpha_n
	\end{pmatrix}
\end{equation}
Normalmente denotaremos por $\mc{B}$ a la matriz fila de vectores de la ecuación \eqref{geovec_eq_combinacionLinealMatrices}. Nótese que esto es un abuso de notación, ya que $\mc{B}$ también denotaba al conjunto de los vectores de la base, sin embargo, el contexto dejará bastante claro a cual de las dos cosas nos referimos. Asimismo, denotaremos por $X_{\mc{B}}$ (o cosas similares) a la matriz columna de coeficientes de la ecuación anterior. De este modo tenemos, más escuetamente
\begin{equation*}
	x=\mc{B}X
\end{equation*}
Si consideramos otra base de $E$, digamos $\mc{B}':=\{e_1',\dots,e_n'\}$, es claro que, al igual que pasaba con $\mc{B}$, todo vector de $E$, admitirá una escritura única en términos de $\mc{B}'$, es decir, para todo $x\in E$ tendremos que $x=\mc{B}'X'$.

Nos planteamos el problema de, dado un vector $x\in E$, averiguar qué relaciones existen entre las dos escrituras de $x$ respecto de $\mc{B}$ y $\mc{B'}$. Es decir, como podemos pasar de $X$ a $X'$ y viceversa. Como siempre en matemáticas, cuando no se tiene ni idea de cómo resolver un problema, lo que hay que hacer es ponerse a escribir obviedades, a ver si se es capaz de obtener algo no del todo obvio.

Como primera obviedad, es claro que los vectores que conforman $\mc{B}'$ son vectores de $E$, luego admiten una escritura única en términos de $\mc{B}$. Escribamos estas escrituras únicas (valga la redundancia)
\begin{equation*}
	\begin{array}{c}
		e_1' = \alpha_1^1e_1 +\dots+\alpha_n^1e_n\\
		\cdots\\
		e_n' = \alpha_1^ne_1 +\dots+\alpha_n^ne_n
	\end{array}
\end{equation*}
Escrito así, nada parece tener sentido, sin embargo, si lo escribimos de forma matricial obtenemos
\begin{equation*}
	\begin{pmatrix}
	e_1'\\
	\cdots\\
	e_n'
	\end{pmatrix}=\begin{pmatrix}
	\alpha_1^1 & \cdots & \alpha_n^1\\
	\cdots & \cdots & \cdots\\
	\alpha_1^n & \cdots & \alpha_n^n
	\end{pmatrix}\begin{pmatrix}
	e_1\\
	\cdots\\
	e_n
	\end{pmatrix}
\end{equation*}
Antes nos aparecieron matrices fila cuyos coeficientes eran los vectores de una base, ahora nos aparecen vectores columna. Como ya introducimos una notación para el anterior caso y sería desagradable introducir más (la memoria humana es escasa), vamos a trasponer toda la ecuación, obteniendo
\begin{equation*}
	\begin{pmatrix}
	e_1' & \cdots & e_n'
	\end{pmatrix}=\begin{pmatrix}
	e_1 & \cdots & e_n
	\end{pmatrix}\begin{pmatrix}
	\alpha_1^1 & \cdots & \alpha_1^n\\
	\cdots & \cdots & \cdots\\
	\alpha_n^1 & \cdots & \alpha_n^n
	\end{pmatrix}
\end{equation*}
Denotando por $P$ a la matriz de coeficientes de la ecuación anterior obtenemos la expresión reducida
\begin{equation*}
	\mc{B}'=\mc{B}P
\end{equation*}
Sabemos que $x$ se escribe en términos de $\mc{B}'$ como $x=\mc{B'}X'$. Por la ecuación anterior tenemos que $x=\mc{B}PX'$. Además, por la ecritura de $x$ respecto de $\mc{B}$ sabemos que $x=\mc{B}X$. Combinando expresiones tenemos que
\begin{equation*}
	\mc{B}X=\mc{B}PX'\sii\mc{B}X-\mc{B}PX'=0\sii\mc{B}(X-PX')=0
\end{equation*}
Si nos fijamos, $X-PX'$ es un vector columna de coeficientes, a los que llamaremos $\beta_1,\dots,\beta_n$, luego tenemos que
\begin{equation*}
	\mc{B}(X-PX')=e_1\beta_1+\dots+e_n\beta_n=0
\end{equation*}
Como $\{e_1,\dots,e_n\}$ es una base, es un conjunto de vectores linealmente independientes, y, por tanto, a la fuerza se tiene que $\beta_1=\dots=\beta_n=0$, por lo que $X-PX'=0$. Dicho de otra forma, se tiene que
\begin{equation*}
	X = PX'
\end{equation*}
Para recordar esta sencilla fórmula, únicamente hay que fijarse en que las columnas de $P$ se corresponden con la escritura de los vectores de $\mc{B}'$ en función de la base $\mc{B}$. A partir de ahora llamaremos a la matriz $P$, \tbi[matriz!de cambio de base]{matriz de cambio de base}.

Es importante notar que $P$ es una matriz invertible, ya que sus columnas son linealmente independientes (por ser $\mc{B}'$ una base de $E$).

Además, como cosa curiosa, se puede demostrar que todos las matrices regulares se corresponden con una matriz de cambio entre dos bases. Demostrarlo no es complicado y se deja al lector.